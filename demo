from __future__ import print_function
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report
from sklearn import metrics
from sklearn import tree
import warnings
warnings.filterwarnings('ignore')
PATH = '/content/Crop_recommendation.csv'
df = pd.read_csv(PATH)
df.head()
df.tail()
df.size
df.shape
df.columns
df['label'].unique()
df.dtypes
df['label'].value_counts()
sns.heatmap(df.corr(),annot=True)
features = df[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]
target = df['label']
labels = df['label']
acc = []
model = []
from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =0, stratify = target)
from sklearn.tree import DecisionTreeClassifier

DecisionTree = DecisionTreeClassifier(criterion="entropy",random_state=2,max_depth=5)

DecisionTree.fit(Xtrain,Ytrain)

predicted_values = DecisionTree.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Decision Tree')
print("DecisionTrees's Accuracy is: ", x*100)

print(classification_report(Ytest,predicted_values))
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
DecisionTree.fit(Xtrain,Ytrain)
predicted_values = DecisionTree.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=DecisionTree.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=DecisionTree.classes_)
fig, ax = plt.subplots(figsize=(25,25))
disp.plot(ax=ax)
plt.show()
from sklearn.model_selection import cross_val_score
score = cross_val_score(DecisionTree, features, target,cv=5)
score
import pickle
# Dump the trained Naive Bayes classifier with Pickle
DT_pkl_filename = 'DecisionTree.pkl'
# Open the file to save as pkl file
DT_Model_pkl = open(DT_pkl_filename, 'wb')
pickle.dump(DecisionTree, DT_Model_pkl)
# Close the pickle instances
DT_Model_pkl.close()
from sklearn.naive_bayes import GaussianNB

NaiveBayes = GaussianNB()

NaiveBayes.fit(Xtrain,Ytrain)

predicted_values = NaiveBayes.predict(Xtest)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Naive Bayes')
print("Naive Bayes's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
NaiveBayes.fit(Xtrain,Ytrain)
predicted_values = NaiveBayes.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=NaiveBayes.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=NaiveBayes.classes_)
fig, ax = plt.subplots(figsize=(25,25))
disp.plot(ax=ax)
plt.show()
score = cross_val_score(NaiveBayes,features,target,cv=5)
score
import pickle
# Dump the trained Naive Bayes classifier with Pickle
NB_pkl_filename = 'NBClassifier.pkl'
# Open the file to save as pkl file
NB_Model_pkl = open(NB_pkl_filename, 'wb')
pickle.dump(NaiveBayes, NB_Model_pkl)
# Close the pickle instances
NB_Model_pkl.close()
from sklearn.svm import SVC

SVM = SVC(gamma='auto')

SVM.fit(Xtrain,Ytrain)

predicted_values = SVM.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('SVM')
print("SVM's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

SVM.fit(Xtrain, Ytrain)

predicted_values = SVM.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=SVM.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=SVM.classes_)

fig, ax = plt.subplots(figsize=(25, 25))
disp.plot(ax=ax)
plt.show()
score = cross_val_score(SVM,features,target,cv=5)
score
from sklearn.linear_model import LogisticRegression

LogReg = LogisticRegression(random_state=2)

LogReg.fit(Xtrain,Ytrain)

predicted_values = LogReg.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('Logistic Regression')
print("Logistic Regression's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
LogReg.fit(Xtrain,Ytrain)
predicted_values = LogReg.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=LogReg.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=LogReg.classes_)
fig, ax = plt.subplots(figsize=(25,25))
disp.plot(ax=ax)
plt.show()
score = cross_val_score(LogReg,features,target,cv=5)
score
import pickle
# Dump the trained Naive Bayes classifier with Pickle
LR_pkl_filename = 'LogisticRegression.pkl'
# Open the file to save as pkl file
LR_Model_pkl = open(DT_pkl_filename, 'wb')
pickle.dump(LogReg, LR_Model_pkl)
# Close the pickle instances
LR_Model_pkl.close()
from sklearn.ensemble import RandomForestClassifier

RF = RandomForestClassifier(n_estimators=20, random_state=0)
RF.fit(Xtrain,Ytrain)

predicted_values = RF.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('RF')
print("RF's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
RF.fit(Xtrain,Ytrain)
predicted_values = RF.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=RF.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=RF.classes_)
fig, ax = plt.subplots(figsize=(25,25))
disp.plot(ax=ax)
plt.show()
score = cross_val_score(RF,features,target,cv=5)
score
import pickle
# Dump the trained Naive Bayes classifier with Pickle
RF_pkl_filename = 'RandomForest.pkl'
# Open the file to save as pkl file
RF_Model_pkl = open(RF_pkl_filename, 'wb')
pickle.dump(RF, RF_Model_pkl)
# Close the pickle instances
RF_Model_pkl.close()
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Ytrain = le.fit_transform(Ytrain)
import xgboost as xgb
XB = xgb.XGBClassifier()
XB.fit(Xtrain,Ytrain)

predicted_values = XB.predict(Xtest)
predicted_values = le.inverse_transform(predicted_values)
x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('XGBoost')
print("XGBoost's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
score = cross_val_score(XB,Xtrain,Ytrain,cv=5)
score
import pickle
# Dump the trained Naive Bayes classifier with Pickle
XB_pkl_filename = 'XGBoost.pkl'
# Open the file to save as pkl file
XB_Model_pkl = open(XB_pkl_filename, 'wb')
pickle.dump(XB, XB_Model_pkl)
# Close the pickle instances
XB_Model_pkl.close()
plt.figure(figsize=[10,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette='dark')
accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =0)
from sklearn.ensemble import GradientBoostingClassifier
grad = GradientBoostingClassifier(n_estimators=20, learning_rate=1.0, max_depth=1, random_state=0).fit(Xtrain, Ytrain)
predicted_values = grad.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('grad')
print("grad's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
import lightgbm as lgb
from lightgbm import LGBMClassifier
mod = LGBMClassifier()
mod.fit(Xtrain, Ytrain)
predicted_values = mod.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('lgb')
print("lgb's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
from sklearn.linear_model import SGDClassifier
sg = SGDClassifier()

sg.fit(Xtrain, Ytrain)
predicted_values = sg.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('sgd')
print("sgd's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
knn.fit(Xtrain, Ytrain)
predicted_values = knn.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
acc.append(x)
model.append('knn')
print("knn's Accuracy is: ", x)

print(classification_report(Ytest,predicted_values))
plt.figure(figsize=[10,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette='dark')
accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v)
from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =0, stratify = target)
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import VotingClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import GaussianNB

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import cross_val_score

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Ytrain = le.fit_transform(Ytrain)

model_1 = LogisticRegression()

model_1.fit(Xtrain, Ytrain)

ypred = model_1.predict(Xtest)

model_2 = GaussianNB()

model_2.fit(Xtrain, Ytrain)

ypred = model_2.predict(Xtest)

model_3 = RandomForestClassifier()

model_3.fit(Xtrain, Ytrain)

ypred = model_3.predict(Xtest)

model_4= XGBClassifier()
model_4.fit(Xtrain,Ytrain)
ypred= model_4.predict(Xtest)
eclf = VotingClassifier(

 estimators=[('lr', model_1), ('rf', model_2), ('gnb', model_3),('xg',model_4)],

 voting='hard'

)
eclf.fit(Xtrain,Ytrain)
predicted_values = eclf.predict(Xtest)
predicted_values = le.inverse_transform(predicted_values)
x = metrics.accuracy_score(Ytest, predicted_values)
print(x)

for clf, label in zip([model_1, model_2, model_3,model_4, eclf], ['Logistic Regression', 'Naive Bayes', 'Random Forest','xgboost', 'Ensemble']):

 scores = cross_val_score(clf, Xtrain, Ytrain, scoring='accuracy', cv=5)

 print("Accuracy: %f (+/- %f) [%s]" % (scores.mean(), scores.std(), label))
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import VotingClassifier

from sklearn.linear_model import LogisticRegression

from sklearn.naive_bayes import GaussianNB

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import cross_val_score

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
Ytrain = le.fit_transform(Ytrain)

model_1 = LogisticRegression()

model_1.fit(Xtrain, Ytrain)

ypred = model_1.predict(Xtest)

model_2 = GaussianNB()

model_2.fit(Xtrain, Ytrain)

ypred = model_2.predict(Xtest)

model_3 = RandomForestClassifier()

model_3.fit(Xtrain, Ytrain)

ypred = model_3.predict(Xtest)

model_4= XGBClassifier()
model_4.fit(Xtrain,Ytrain)
ypred= model_4.predict(Xtest)
eclf = VotingClassifier(

 estimators=[('lr', model_1), ('rf', model_2), ('gnb', model_3),('xg',model_4)],

 voting='hard'

)
eclf.fit(Xtrain,Ytrain)
predicted_values = eclf.predict(Xtest)
predicted_values = le.inverse_transform(predicted_values)
x = metrics.accuracy_score(Ytest, predicted_values)
print(x)

for clf, label in zip([model_1, model_2, model_3,model_4, eclf], ['Logistic Regression', 'Naive Bayes', 'Random Forest','xgboost', 'Ensemble']):

 scores = cross_val_score(clf, Xtrain, Ytrain, scoring='accuracy', cv=5)

 print("Accuracy: %f (+/- %f) [%s]" % (scores.mean(), scores.std(), label))
from sklearn.model_selection import train_test_split
Xtrain, Xtest, Ytrain, Ytest = train_test_split(features,target,test_size = 0.2,random_state =0, stratify = target)
from sklearn import metrics
from sklearn.metrics import classification_report
from lightgbm.compat import check_classification_targets
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

model_1 = LogisticRegression()
model_1.fit(Xtrain, Ytrain)
ypred1 = model_1.predict(Xtest)

model_2 = GaussianNB()
model_2.fit(Xtrain, Ytrain)
ypred2 = model_2.predict(Xtest)

model_3 = RandomForestClassifier()
model_3.fit(Xtrain, Ytrain)
ypred3 = model_3.predict(Xtest)

eclf = VotingClassifier(
    estimators=[('lr', model_1), ('rf', model_2), ('gnb', model_3)],
    voting='hard'
)

eclf.fit(Xtrain, Ytrain)
predicted_values = eclf.predict(Xtest)

x = metrics.accuracy_score(Ytest, predicted_values)
print(x)
# acc.append(x)
# model.append('ensemble')

for clf, label in zip([model_1, model_2, model_3, eclf], ['Logistic Regression', 'Naive Bayes', 'Random Forest', 'Ensemble']):
    scores = cross_val_score(clf, features, target, scoring='accuracy', cv=5)
    print("Accuracy: %f (+/- %f) [%s]" % (scores.mean(), scores.std(), label))

print(classification_report(Ytest, predicted_values))
data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])
prediction = eclf.predict(data)
print(prediction)
data = np.array([[104,18, 30, 23.603016, 60.3, 6.7, 140.91]])
prediction = eclf.predict(data)
print(prediction)
from sklearn.metrics import confusion_matrix
 
cm = confusion_matrix(Ytest,predicted_values)
print(cm)
from sklearn.metrics import confusion_matrix
 
cm = confusion_matrix(Ytest,predicted_values)
print(cm) 
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
eclf.fit(Xtrain,Ytrain)
predicted_values = eclf.predict(Xtest)
cm = confusion_matrix(Ytest, predicted_values, labels=eclf.classes_)
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=eclf.classes_)
fig, ax = plt.subplots(figsize=(25,25))
disp.plot(ax=ax)
plt.show()
plt.figure(figsize=[11,5],dpi = 100)
plt.title('Accuracy Comparison')
plt.xlabel('Accuracy')
plt.ylabel('Algorithm')
sns.barplot(x = acc,y = model,palette='dark')
accuracy_models = dict(zip(model, acc))
for k, v in accuracy_models.items():
    print (k, '-->', v)
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import f1_score
from sklearn.metrics import accuracy_score
eclf.fit(Xtrain,Ytrain)
predicted_values = eclf.predict(Xtest)
a=accuracy_score(Ytest,predicted_values)
p=precision_score(Ytest,predicted_values , labels=eclf.classes_, average='micro')
r=recall_score(Ytest,predicted_values , labels=eclf.classes_, average='micro')
f=f1_score (Ytest,predicted_values , labels=eclf.classes_, average='micro')
print(a,p,r,f)
